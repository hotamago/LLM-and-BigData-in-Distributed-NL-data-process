name: hota-project-3

services:
  # Client Service
  client:
    image: hotamago/project-3/client:latest
    build:
      context: ../client
      dockerfile: Dockerfile
    container_name: client-container
    ports:
      - "3005:3005" # Streamlit UI port
    networks:
      - spark-hadoop-net
    volumes:
      - ../client:/app # Mount the client directory to the container for development
      - client_cache:/app/.cache
      - hadoop_namenode_bin:/opt/hadoop/bin
    environment:
      - mode=prod
      - HADOOP_HOME=/opt/hadoop
      - hadoop.home.dir=/opt/hadoop
      # Pass Postgres credentials into client
      - POSTGRES_USER=${POSTGRES_USER:-user}
      - POSTGRES_PASSWORD=${POSTGRES_PASSWORD:-password}
      - POSTGRES_DB=${POSTGRES_DB:-project3_db}
      # Database URL must include password
      - DATABASE_URL=postgresql://${POSTGRES_USER:-user}:${POSTGRES_PASSWORD:-password}@postgres:5432/${POSTGRES_DB:-project3_db}
    depends_on:
      - spark-master
      - hadoop-namenode
      - postgres # Add dependency on postgres
      - migrate # Ensure migrations run before client starts

  # Spark Master Service
  spark-master:
    hostname: spark-master
    image: hotamago/project-3/spark:3.5.3
    build:
      context: ../spark
      dockerfile: Dockerfile
    container_name: spark-master
    env_file:
      - ./spark-master.env
    ports:
      - "8080:8080"   # Spark Master UI
      - "7077:7077"   # Spark Master port
    volumes:
      - spark_master_data:/masternode/
    deploy:
      resources:
        limits:
          cpus: '1.0'
          memory: '2G'
    networks:
      - spark-hadoop-net

  # Spark Worker 1
  spark-worker-01:
    image: hotamago/project-3/spark:3.5.3
    build:
      context: ../spark
      dockerfile: Dockerfile
    container_name: spark-worker-01
    environment:
      SPARK_WORKER_MEMORY: 1G
      SPARK_WORKER_CORES: 1
    env_file:
      - ./spark-worker.env
    volumes:
      - spark_worker01_data:/workernode-01/
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: '1G'
    networks:
      - spark-hadoop-net
    depends_on:
      - spark-master
    # ports: # for debugging
    #   - "8081:8081"

  # Spark Worker 2
  spark-worker-02:
    image: hotamago/project-3/spark:3.5.3
    build:
      context: ../spark
      dockerfile: Dockerfile
    container_name: spark-worker-02
    environment:
      SPARK_WORKER_MEMORY: 1G
      SPARK_WORKER_CORES: 1
    env_file:
      - ./spark-worker.env
    volumes:
      - spark_worker02_data:/workernode-02/
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: '1G'
    networks:
      - spark-hadoop-net
    depends_on:
      - spark-master
    # ports: # for debugging
    #   - "8082:8081"

  # Spark Worker 3
  spark-worker-03:
    image: hotamago/project-3/spark:3.5.3
    build:
      context: ../spark
      dockerfile: Dockerfile
    container_name: spark-worker-03
    environment:
      SPARK_WORKER_MEMORY: 1G
      SPARK_WORKER_CORES: 1
    env_file:
      - ./spark-worker.env
    volumes:
      - spark_worker03_data:/workernode-03/
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: '1G'
    networks:
      - spark-hadoop-net
    depends_on:
      - spark-master
    # ports: # for debugging
    #   - "8083:8081"

  # Spark Worker 4
  spark-worker-04:
    image: hotamago/project-3/spark:3.5.3
    build:
      context: ../spark
      dockerfile: Dockerfile
    container_name: spark-worker-04
    environment:
      SPARK_WORKER_MEMORY: 1G
      SPARK_WORKER_CORES: 1
    env_file:
      - ./spark-worker.env
    volumes:
      - spark_worker04_data:/workernode-04/
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: '1G'
    networks:
      - spark-hadoop-net
    depends_on:
      - spark-master
    # ports: # for debugging
    #   - "8084:8081"

  # Spark Worker 5
  # spark-worker-05:
  #   image: hotamago/project-3/spark:3.5.3
  #   build:
  #     context: ../spark
  #     dockerfile: Dockerfile
  #   container_name: spark-worker-05
  #   environment:
  #     SPARK_WORKER_MEMORY: 1G
  #     SPARK_WORKER_CORES: 1
  #   env_file:
  #     - ./spark-worker.env
  #   volumes:
  #     - spark_worker05_data:/workernode-05/
  #   deploy:
  #     resources:
  #       limits:
  #         cpus: '0.5'
  #         memory: '1G'
  #   networks:
  #     - spark-hadoop-net
  #   depends_on:
  #     - spark-master
  #   # ports: # for debugging
  #   #   - "8085:8081"

  # # Spark Worker 6
  # spark-worker-06:
  #   image: hotamago/project-3/spark:3.5.3
  #   build:
  #     context: ../spark
  #     dockerfile: Dockerfile
  #   container_name: spark-worker-06
  #   environment:
  #     SPARK_WORKER_MEMORY: 1G
  #     SPARK_WORKER_CORES: 1
  #   env_file:
  #     - ./spark-worker.env
  #   volumes:
  #     - spark_worker06_data:/workernode-06/
  #   deploy:
  #     resources:
  #       limits:
  #         cpus: '0.5'
  #         memory: '1G'
  #   networks:
  #     - spark-hadoop-net
  #   depends_on:
  #     - spark-master
  #   # ports: # for debugging
  #   #   - "8086:8081"

  # # Spark Worker 7
  # spark-worker-07:
  #   image: hotamago/project-3/spark:3.5.3
  #   build:
  #     context: ../spark
  #     dockerfile: Dockerfile
  #   container_name: spark-worker-07
  #   environment:
  #     SPARK_WORKER_MEMORY: 1G
  #     SPARK_WORKER_CORES: 1
  #   env_file:
  #     - ./spark-worker.env
  #   volumes:
  #     - spark_worker07_data:/workernode-07/
  #   deploy:
  #     resources:
  #       limits:
  #         cpus: '0.5'
  #         memory: '1G'
  #   networks:
  #     - spark-hadoop-net
  #   depends_on:
  #     - spark-master
  #   # ports: # for debugging
  #   #   - "8087:8081"

  # # Spark Worker 8
  # spark-worker-08:
  #   image: hotamago/project-3/spark:3.5.3
  #   build:
  #     context: ../spark
  #     dockerfile: Dockerfile
  #   container_name: spark-worker-08
  #   environment:
  #     SPARK_WORKER_MEMORY: 1G
  #     SPARK_WORKER_CORES: 1
  #   env_file:
  #     - ./spark-worker.env
  #   volumes:
  #     - spark_worker08_data:/workernode-08/
  #   deploy:
  #     resources:
  #       limits:
  #         cpus: '0.5'
  #         memory: '1G'
  #   networks:
  #     - spark-hadoop-net
  #   depends_on:
  #     - spark-master
  #   # ports: # for debugging
  #   #   - "8088:8081"

  # Hadoop NameNode
  hadoop-namenode:
    image: hotamago/project-3/hadoop:3.4.1
    build:
      context: ../hadoop
      dockerfile: Dockerfile
    container_name: hadoop-namenode
    hostname: namenode
    command: [ "hdfs", "namenode" ]
    # command: [ "bash", "-c", "hdfs namenode -format -y && hdfs namenode" ]
    ports:
      - "9870:9870"  # NameNode UI
      - "9000:9000"  # HDFS port
    env_file:
      - ./hadoop.env
    volumes:
      - hadoop_namenode_data:/hadoop/name
      - hadoop_namenode_data:/data
      - hadoop_namenode_bin:/opt/hadoop/bin
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: '2G'
    networks:
      - spark-hadoop-net

  # Hadoop DataNode 1
  hadoop-datanode-01:
    image: hotamago/project-3/hadoop:3.4.1
    build:
      context: ../hadoop
      dockerfile: Dockerfile
    container_name: hadoop-datanode-01
    hostname: datanode-01
    command: [ "hdfs", "datanode" ]
    depends_on:
      - hadoop-namenode
    env_file:
      - ./hadoop.env
    volumes:
      - hadoop_datanode01_data:/hadoop/data
      - hadoop_datanode01_data:/data
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: '2G'
    networks:
      - spark-hadoop-net

  # Hadoop DataNode 02
  hadoop-datanode-02:
    image: hotamago/project-3/hadoop:3.4.1
    build:
      context: ../hadoop
      dockerfile: Dockerfile
    container_name: hadoop-datanode-02
    hostname: datanode-02
    command: [ "hdfs", "datanode" ]
    depends_on:
      - hadoop-namenode
    env_file:
      - ./hadoop.env
    volumes:
      - hadoop_datanode02_data:/hadoop/data
      - hadoop_datanode02_data:/data
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: '2G'
    networks:
      - spark-hadoop-net

  hadoop-datanode-03:
    image: hotamago/project-3/hadoop:3.4.1
    build:
      context: ../hadoop
      dockerfile: Dockerfile
    container_name: hadoop-datanode-03
    hostname: datanode-03
    command: [ "hdfs", "datanode" ]
    depends_on:
      - hadoop-namenode
    env_file:
      - ./hadoop.env
    volumes:
      - hadoop_datanode03_data:/hadoop/data
      - hadoop_datanode03_data:/data
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: '2G'
    networks:
      - spark-hadoop-net
      
  # PostgreSQL Database Service
  postgres:
    image: postgres:15-alpine
    container_name: postgres-db
    hostname: postgres
    environment:
      POSTGRES_DB: ${POSTGRES_DB:-project3_db}
      POSTGRES_USER: ${POSTGRES_USER:-user}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD:-password}
    volumes:
      - postgres_data:/var/lib/postgresql/data
    ports:
      - "5432:5432"
    networks:
      - spark-hadoop-net
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: '2G'

  # Migration Service (using client image for simplicity, assumes Alembic is installed there)
  migrate:
    image: hotamago/project-3/client:latest # Reuse client image if it has necessary tools
    build:
      context: ../client
      dockerfile: Dockerfile # Assumes client Dockerfile installs DB drivers & Alembic
    container_name: migrate-container
    working_dir: /app
    entrypoint: /bin/sh # Override the entrypoint
    command: -c "alembic upgrade head" # Command to run migrations
    environment:
      - POSTGRES_USER=${POSTGRES_USER:-user}
      - POSTGRES_PASSWORD=${POSTGRES_PASSWORD:-password}
      - POSTGRES_DB=${POSTGRES_DB:-project3_db}
      - DATABASE_URL=postgresql://${POSTGRES_USER:-user}:${POSTGRES_PASSWORD:-password}@postgres:5432/${POSTGRES_DB:-project3_db}
    volumes:
      - ../client:/app # Mount client code to access migration scripts
    networks:
      - spark-hadoop-net
    depends_on:
      - postgres # Ensure postgres is ready before running migrations

volumes:
  # Client Volumes
  client_cache:
    driver: local

  # Spark Volumes
  spark_master_data:
    driver: local
  spark_worker01_data:
    driver: local
  spark_worker02_data:
    driver: local
  spark_worker03_data:
    driver: local
  spark_worker04_data:
    driver: local
  spark_worker05_data:
    driver: local
  spark_worker06_data:
    driver: local
  spark_worker07_data:
    driver: local
  spark_worker08_data:
    driver: local

  # Hadoop Volumes
  hadoop_namenode_data:
    driver: local
  hadoop_namenode_bin:
    driver: local
  hadoop_datanode01_data:
    driver: local
  hadoop_datanode02_data:
    driver: local
  hadoop_datanode03_data:
    driver: local
  hadoop_datanode04_data:
    driver: local

  # PostgreSQL Volume
  postgres_data:
    driver: local

networks:
  spark-hadoop-net:
    driver: bridge
